{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "# Sentiment Analysis IMDB Dataset\n",
    "\n",
    "This program attains 90% accuracy on sentiment classification (positive/negative) of the IMDB dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp_proj_utils as utils\n",
    "\n",
    "# Function for loading imdb dataset\n",
    "def load_imdb():\n",
    "    train, test = utils.get_imdb_dataset()\n",
    "    TEXT_COL, LABEL_COL = 'text', 'sentiment'\n",
    "    return (\n",
    "        train[TEXT_COL], train[LABEL_COL],\n",
    "        test[TEXT_COL], test[LABEL_COL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data already available, skip downloading.\n",
      "imdb loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "train_text, train_label, test_text, test_label = load_imdb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Shape, should not throw exceptions\n",
    "for data in train_text, train_label, test_text, test_label:\n",
    "    assert data.shape == (25000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data \n",
    "\n",
    "### Build Vectorizer\n",
    "\n",
    "We use the build-in method for generating vocabulary and doing vectorization provided by `TfidfVectorizer` from Sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    min_df=2, # ignore word that only appears in 1 document\n",
    "    ngram_range=(1, 2), # consider both uni-gram and bi-gram\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn (fit) and transform text into vector\n",
    "train_x = tfidf_vectorizer.fit_transform(train_text)\n",
    "\n",
    "# Convert label to 0 and 1 (optional)\n",
    "train_y = train_label.apply(lambda x: 1 if x == 'pos' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training x shape: (25000, 438350)\n",
      "Training y shape: (25000,)\n"
     ]
    }
   ],
   "source": [
    "# Check the shape\n",
    "print('Training x shape:', train_x.shape)\n",
    "print('Training y shape:', train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the very high dimensionality of the training set. We will use word embeddings to resolve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    12500\n",
       "0    12500\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expect 12500 for 1 and 0, instead of pos and neg\n",
    "train_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same transformer to validation set as well\n",
    "# Simply call `transform` this time, don't do `fit` again\n",
    "test_x = tfidf_vectorizer.transform(test_text)\n",
    "test_y = test_label.apply(lambda x: 1 if x == 'pos' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "assert test_x.shape == train_x.shape\n",
    "assert test_y.shape == train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction\n",
    "\n",
    "There are a couple of ways we could reduce the dimensionality.\n",
    "\n",
    "- One is to set a hard cut off on the number words in our vocabulary. For instance, keep the top 3000 frequent words in the vocabulary. \n",
    "- The other way is to use a pretrained word embeddings. This is based on transfer learning, where we reduce the dimensions by learning the relation between words from another training task (e.g.: Word2Vec). \n",
    "\n",
    "Pros and Cons of the Two Methods: \n",
    "\n",
    "- One defect about the first approach is that it didn't make use of the labels, i.e.: the dimensionality reduction was applied solely on information about the text. \"Top words are more useful\" is nothing more than one of our hypothesis. We have no idea on what words are more useful when doing classification.\n",
    "- The second approach is better. But one potential problem is that we kept it fixed, meaning that it may not fit in to our problem very well. One solution is to re-train the word embedding together with the weights of the network (simply set `trainable=True`). The training may take a long time thus a GPU is highly recommended.\n",
    "\n",
    "Here, we decided to use another more general way to reduce dimensionality. We will be using `SelectKBest` from `sklearn` and using `f_classif` to help up pick up k best features (word). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 20000 # Dimensions to keep, a hyper parameter\n",
    "\n",
    "# Create a feature selector\n",
    "# By default, f_classif algorithm is used\n",
    "# Other available options include mutual_info_classif, chi2, f_regression etc. \n",
    "\n",
    "selector = SelectKBest(k=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectKBest(k=20000, score_func=<function f_classif at 0x7fe048f92560>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The feature selector also requires information from labels\n",
    "# Fit on training data\n",
    "selector.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to both training data and testing data\n",
    "train_x = selector.transform(train_x)\n",
    "test_x = selector.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "assert train_x.shape == (25000, 20000)\n",
    "assert test_x.shape == (25000, 20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a MLP Model\n",
    "\n",
    "We use a Multi-Layer Perceptron model, aka Feed Forward Network, which is quite ubiquitous in applications and robust. Deeper networks are usually more powerful, but they are usually more data hungry. Since we don't have much data, a MLP model may work better in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Input, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is quite similar to `build_nn_model` in previous notebook,\n",
    "# except that we add an extra dropout layer after each dense layer\n",
    "\n",
    "def build_mlp_model(input_dim, layers, output_dim, dropout_rate=0.2):\n",
    "    # Input layer\n",
    "    X = Input(shape=(input_dim,))\n",
    "    \n",
    "    # Hidden layer(s)\n",
    "    H = X\n",
    "    for layer in layers:\n",
    "        H = Dense(layer, activation='relu')(H)\n",
    "        H = Dropout(rate=dropout_rate)(H)\n",
    "    \n",
    "    # Output layer\n",
    "    activation_func = 'softmax' if output_dim > 1 else 'sigmoid'\n",
    "    \n",
    "    Y = Dense(output_dim, activation=activation_func)(H)\n",
    "    return Model(inputs=X, outputs=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = {\n",
    "    'learning_rate': 1e-3,  # default for Adam\n",
    "    'epochs': 1000,\n",
    "    'batch_size': 64,\n",
    "    'layers': [64, 32],\n",
    "    'dim': DIM,\n",
    "    'dropout_rate': 0.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 20000)]           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                1280064   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,282,177\n",
      "Trainable params: 1,282,177\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp_model = build_mlp_model(\n",
    "    input_dim=hyper_params['dim'],\n",
    "    layers=hyper_params['layers'],\n",
    "    output_dim=1,\n",
    "    dropout_rate=hyper_params['dropout_rate'],\n",
    ")\n",
    "\n",
    "mlp_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "mlp_model.compile(\n",
    "    optimizer=Adam(lr=hyper_params['learning_rate']),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['acc'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks\n",
    "\n",
    "Callbacks (aka hooks) are functions called every N epochs that help you monitor and log the training process. By default, they will be called every 1 epoch. We will be using two common callbacks here: `EarlyStopping` and `ModelCheckpoint`. The first is used to prevent overfitting and the second is used to keep track of the best models we got so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stoppping_hook = EarlyStopping(\n",
    "    monitor='val_loss',  # what metrics to track\n",
    "    patience=2,  # maximum number of epochs allowed without imporvement on monitored metrics \n",
    ")\n",
    "\n",
    "CPK_PATH = 'model_cpk.hdf5'    # path to store checkpoint\n",
    "\n",
    "model_cpk_hook = ModelCheckpoint(\n",
    "    CPK_PATH,\n",
    "    monitor='val_loss', \n",
    "    save_best_only=True,  # Only keep the best model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model, Hope for the Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 5s 182us/sample - loss: 0.3557 - acc: 0.8564 - val_loss: 0.2330 - val_acc: 0.9059\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 4s 173us/sample - loss: 0.1424 - acc: 0.9513 - val_loss: 0.2492 - val_acc: 0.9012\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 4s 173us/sample - loss: 0.0832 - acc: 0.9747 - val_loss: 0.2940 - val_acc: 0.8943\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "his = mlp_model.fit(\n",
    "    train_x, \n",
    "    train_y, \n",
    "    epochs=10,\n",
    "    validation_data=[test_x, test_y],\n",
    "    batch_size=hyper_params['batch_size'],\n",
    "    callbacks=[early_stoppping_hook, model_cpk_hook],\n",
    ")\n",
    "\n",
    "print('Training finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Load the best model and do evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 2s 64us/sample - loss: 0.2330 - acc: 0.9059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.23295250891685484, 0.90588]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model checkpoint\n",
    "mlp_model.load_weights(CPK_PATH)\n",
    "\n",
    "# Accuracy on validation \n",
    "mlp_model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are similar to the following screenshot (benchmark run). The exact number may be different as there is some randomization introduced during batching.\n",
    "\n",
    "<img src='resources/example_validation.png'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
